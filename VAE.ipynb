{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "VAE.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNAOM9ggEUCNulJchbJU+FX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JunruL/Variational-Autoencoder/blob/main/VAE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fetch the Data"
      ],
      "metadata": {
        "id": "Vi_jekCpax91"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir ~/.kaggle\n",
        "!mv kaggle.json ~/.kaggle\n",
        "!kaggle datasets download -d playlist/mnistzip\n",
        "!unzip mnistzip.zip"
      ],
      "metadata": {
        "id": "o1jSMZe9Qs9N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "ZmhSl4Iia54E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor\n",
        "from torch.utils.data import DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import cv2 as cv\n",
        "import numpy as np\n",
        "from torch import nn"
      ],
      "metadata": {
        "id": "VtKXoqbiShbj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creat a Custom Dataset"
      ],
      "metadata": {
        "id": "IhtlwOx1a-7x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomImageDataset(Dataset):\n",
        "    def __init__(self, data_lst: list, label_lst: list):\n",
        "        self.data_lst = data_lst\n",
        "        self.label_lst = label_lst\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data_lst)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = cv.imread(data_lst[idx], 0)\n",
        "        image = np.expand_dims(image, axis=2)\n",
        "        image = ToTensor()(image)\n",
        "        label = label_lst[idx]\n",
        "        return image, label"
      ],
      "metadata": {
        "id": "M980ii0JSVqS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_lst = []\n",
        "label_lst = []\n",
        "for number in range(10):\n",
        "  number_lst = os.listdir('./mnist_png/train/' + str(number))\n",
        "  data_lst += ['./mnist_png/train/' + str(number) + '/' + str(file) for file in number_lst]\n",
        "  label_lst += [number for _ in range(len(number_lst))]"
      ],
      "metadata": {
        "id": "Dg1yDDxIUoE2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = CustomImageDataset(number_lst, label_lst)\n",
        "idx = 10000\n",
        "image, label = dataset[idx]\n",
        "print(label)"
      ],
      "metadata": {
        "id": "i8QE6jSLYn_M",
        "outputId": "e3b55903-c506-47dd-e9a4-748a99a1963f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
        "for data in train_dataloader:\n",
        "  sample_data, sample_label = data\n",
        "  print(sample_data.shape)\n",
        "  print(sample_label.shape)"
      ],
      "metadata": {
        "id": "uXCzXBDHvHwq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Build Neural Networks"
      ],
      "metadata": {
        "id": "ZMMp1Val8RLJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inference Model (Encoder / recognition Model): \n",
        "\n",
        "$$q_{\\phi} (z | x)$$\n",
        "\n",
        "We use a multivariate normal distribution for it: \n",
        "\n",
        "$$q_{\\phi} (z | x) = \\mathcal{N} (\\vec{z}; \\vec{\\mu}, \\text{diag}(\\vec{\\sigma}))$$\n",
        "\n",
        ", which means\n",
        "\n",
        "$$z \\sim \\mathcal{N} (\\vec{\\mu}, \\text{diag}(\\vec{\\sigma}))$$\n",
        "\n",
        "To make the the encoder differentiable, we can use the reparameterization trick: \n",
        "\n",
        "$$\\vec{\\epsilon} \\sim \\mathcal{N} (0, 1)$$\n",
        "\n",
        "$$(\\vec{\\mu}, \\log \\vec{\\sigma}) = \\text{EncoderNeuralNet}_{\\phi} (\\vec{x})$$\n",
        "\n",
        "$$\\vec{z} = \\vec{\\mu} + \\vec{\\sigma} \\odot \\vec{\\epsilon} $$\n",
        "\n",
        ", where $\\odot$ is the element-wise product."
      ],
      "metadata": {
        "id": "oDOfDkARunGu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.layer1 = nn.Linear(28*28, 512)\n",
        "        self.layer2 = nn.Linear(512, 256)\n",
        "        self.layer3 = nn.Linear(256, 128)\n",
        "        self.mean_layer = nn.Linear(128, 128)\n",
        "        self.log_sd_layer = nn.Linear(128, 128)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        x = self.layer1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.relu(x)\n",
        "        mean = self.mean_layer(x)\n",
        "        log_sd = self.log_sd_layer(x)\n",
        "        sd = torch.exp(log_sd)\n",
        "\n",
        "        # random vector from Normal distribution\n",
        "        epsilon = torch.randn_like(sd)\n",
        "        z = mean + sd * epsilon\n",
        "        return mean, sd, log_sd, z"
      ],
      "metadata": {
        "id": "dVJawoHpyB6-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Decoder, self).__init__()   \n",
        "        self.layer1 = nn.Linear(128, 256)\n",
        "        self.layer2 = nn.Linear(256, 512)\n",
        "        self.layer3 = nn.Linear(512, 28*28)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, z):\n",
        "        z = self.layer1(z)\n",
        "        z = self.relu(z)\n",
        "        z = self.layer2(z)\n",
        "        z = self.relu(z)\n",
        "        z = self.layer3(z)\n",
        "        z = self.relu(z)\n",
        "\n",
        "        x_prime = torch.reshape(z, (z.shape[0], 1, 28, 28))\n",
        "        return x_prime"
      ],
      "metadata": {
        "id": "Ito1AfT26fjE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class VAE(nn.Module):\n",
        "    def __init__(self):\n",
        "      super(VAE, self).__init__()   \n",
        "      self.encoder = Encoder()\n",
        "      self.decoder = Decoder()\n",
        "\n",
        "    def forward(self, x):\n",
        "      mean, sd, log_sd, z = self.encoder(x)\n",
        "      x_prime = self.decoder(z)\n",
        "      return x_prime, mean, sd, log_sd"
      ],
      "metadata": {
        "id": "9ODAIzUCvX9D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for data in train_dataloader:\n",
        "  sample_data, sample_label = data\n",
        "  break"
      ],
      "metadata": {
        "id": "iRqOLW9_y1um"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder = Encoder()\n",
        "mean, sd, log_sd, z = encoder(sample_data)\n",
        "z.shape"
      ],
      "metadata": {
        "id": "g057JADL5wu4",
        "outputId": "94123aa6-edea-43dc-b18a-7e86e727e16d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([32, 128])"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "decoder = Decoder()\n",
        "x_prime = decoder(z)\n",
        "x_prime.shape"
      ],
      "metadata": {
        "id": "KDIEf0cp8zmx",
        "outputId": "5509d34a-8fd2-4f90-e8c8-489269233c64",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([32, 1, 28, 28])"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_img(decoder: Decoder, z: np.array):\n",
        "    \"\"\"Note: the batchsize of z = 1\n",
        "    \"\"\"\n",
        "    return decoder(z)[0][0].detach().numpy()\n",
        "\n",
        "img = generate_img(decoder, torch.randn(1, 128))\n",
        "plt.imshow(img, cmap='gray')"
      ],
      "metadata": {
        "id": "sbU5spfIuKLj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ELBO (Evidence Lower Bound)"
      ],
      "metadata": {
        "id": "YIPCattQHcBY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evidence lower bound (also called variational lower bound), is the optimization objective of the VAE. For any inference model $q_{\\phi}(z|x)$, \n",
        "\n",
        "\\begin{align*}\n",
        "\\log p_{\\theta}(x) &= \\mathbb{E}_{q_{\\phi}(z|x)} \\big[\\log p_{\\theta}(x) \\big]\\\\\n",
        "&= \\mathbb{E}_{q_{\\phi}(z|x)} \\Big[\\log \\frac{p_{\\theta}(x, z)}{p_{\\theta}(z|x)} \\Big]\\\\\n",
        "&= \\mathbb{E}_{q_{\\phi}(z|x)} \\Bigg[\\log \\Big[ \\frac{p_{\\theta}(x, z)}{p_{\\phi}(z|x)} \\frac{p_{\\phi}(z|x)}{p_{\\theta}(z|x)}\\Big] \\Bigg]\\\\\n",
        "&= \\mathbb{E}_{q_{\\phi}(z|x)} \\Big[\\log \\frac{p_{\\theta}(x, z)}{p_{\\phi}(z|x)} \\Big] + \\mathbb{E}_{q_{\\phi}(z|x)} \\Big[\\log \\frac{p_{\\phi}(z|x)}{p_{\\theta}(z|x)} \\Big]\\\\\n",
        "&= \\mathcal{L}_{\\theta, \\phi}(x) + D_{KL}(q_{\\phi}(z|x) || q_{\\theta}(z|x))\\\\\n",
        "&\\geq \\mathcal{L}_{\\theta, \\phi}(x) \\tag{since $D_{KL}(q_{\\phi}(z|x) || q_{\\theta}(z|x)) \\geq 0$}\n",
        "\\end{align*}\n",
        "\n",
        ", where \n",
        "*   $\\mathcal{L}_{\\theta, \\phi}(x)$ is the variational lower bound or the evidence lower bound (ELBO)\n",
        "*   $D_{KL}(q_{\\phi}(z|x) || q_{\\theta}(z|x))$ is the (KL) divergence between $q_{\\phi}(z|x)$ and $q_{\\theta}(z|x)$\n",
        "\n",
        "\n",
        "Form the derivation above, we can see that the ELBO is a lower bound on the log-likeihood of the data. Also, since $$\\mathcal{L}_{\\theta, \\phi}(x) = \\log p_{\\theta}(x) - D_{KL}(q_{\\phi}(z|x) || q_{\\theta}(z|x))$$,\n",
        "maximizing the ELBO $\\mathcal{L}_{\\theta, \\phi}(x)$ w.r.t. the parameters $\\theta$ and $\\phi$ will concurrently\n",
        "\n",
        "*   maximize the marginal likelihood $p_{\\theta}(x)$\n",
        "*   minimize the KL divergence of the approximation $q_{\\phi}(z|x)$ from the true posterior $p_{\\theta}(z|x)$\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "84bn7SCAHqRs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_elbo(x, x_prime, mu, sigma, log_sigma):\n",
        "    '''\n",
        "    - x: data\n",
        "    - x_prime: the reconstructed data\n",
        "    - mu: the mean of the latent distribution\n",
        "    - sigma: the standard deviation of the latent distribution\n",
        "    - log_sigma: the logrithm of sigma\n",
        "\n",
        "    ELBO = -MSE - KL\n",
        "    '''\n",
        "    mse = nn.MSELoss()(x, x_prime)\n",
        "    kl = -1 * log_sigma + 0.5 * (torch.pow(sigma, 2) + torch.pow(mu, 2)) - 0.5\n",
        "    elbo = -1 * mse - torch.sum(kl) / 32\n",
        "\n",
        "    return elbo\n"
      ],
      "metadata": {
        "id": "WP3eArKXcYs4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "elbo = compute_elbo(sample_data, x_prime, mean, sd, log_sd)\n",
        "print(elbo)"
      ],
      "metadata": {
        "id": "thitRbkIj_bL",
        "outputId": "6e48e609-c699-49e6-8a7b-2a5ef9dca0c0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(-0.7611, grad_fn=<SubBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training"
      ],
      "metadata": {
        "id": "Gyye_fLim-TT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataloader = DataLoader(dataset, batch_size=32, shuffle=True, drop_last=True)\n",
        "vae = VAE()\n",
        "\n",
        "learning_rate = 0.001\n",
        "optimizer = torch.optim.SGD(vae.parameters(), lr=learning_rate)\n",
        "\n",
        "epochs = 30\n",
        "for epoch in range(epochs):\n",
        "  for data in train_dataloader:\n",
        "    sample_data, _ = data\n",
        "    x_prime, mean, sd, log_sd = vae(sample_data)\n",
        "    elbo = compute_elbo(sample_data, x_prime, mean, sd, log_sd)\n",
        "    print(elbo)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    elbo.backward()\n",
        "    optimizer.step()\n",
        "    "
      ],
      "metadata": {
        "id": "zoB00Ow5nACx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}